{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f0b58f4",
   "metadata": {},
   "source": [
    "THI·∫æT L·∫¨P V√Ä C√ÄI ƒê·∫∂T TH∆Ø VI·ªÜN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "715c247a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ª≠ d·ª•ng thi·∫øt b·ªã: cpu\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# üîß Import Th∆∞ Vi·ªán\n",
    "# ===============================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "import spacy\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import os\n",
    "\n",
    "# ===============================\n",
    "# ‚öôÔ∏è Thi·∫øt l·∫≠p ban ƒë·∫ßu\n",
    "# ===============================\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {device}\")\n",
    "\n",
    "# ===============================\n",
    "# ‚öôÔ∏è H·∫±ng s·ªë & Token ƒë·∫∑c bi·ªát\n",
    "# ===============================\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_tokens = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "MAX_SENTENCE_LEN = 50\n",
    "DATA_DIR = 'data/'\n",
    "\n",
    "# ===============================\n",
    "# üß† T·∫£i Tokenizer spaCy thay th·∫ø torchtext\n",
    "# ===============================\n",
    "# N·∫øu ch∆∞a t·∫£i, m·ªü terminal v√† ch·∫°y:\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download fr_core_news_sm\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "spacy_fr = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"Tokenizer ti·∫øng Anh\"\"\"\n",
    "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_fr(text):\n",
    "    \"\"\"Tokenizer ti·∫øng Ph√°p\"\"\"\n",
    "    return [tok.text.lower() for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "# ===============================\n",
    "# üß© T·∫°o Vocabulary (thay build_vocab_from_iterator)\n",
    "# ===============================\n",
    "def build_vocab(sentences, tokenizer, max_size=MAX_VOCAB_SIZE):\n",
    "    counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        counter.update(tokenizer(sentence))\n",
    "\n",
    "    # Gi·ªõi h·∫°n k√≠ch th∆∞·ªõc vocab\n",
    "    most_common = counter.most_common(max_size - len(special_tokens))\n",
    "    vocab = {word: idx + len(special_tokens) for idx, (word, _) in enumerate(most_common)}\n",
    "    # Th√™m token ƒë·∫∑c bi·ªát\n",
    "    for idx, token in enumerate(special_tokens):\n",
    "        vocab[token] = idx\n",
    "    return vocab\n",
    "\n",
    "# ===============================\n",
    "# üîÅ H√†m chuy·ªÉn vƒÉn b·∫£n sang ID\n",
    "# ===============================\n",
    "def encode_sentence(sentence, vocab, tokenizer, max_len=MAX_SENTENCE_LEN):\n",
    "    tokens = tokenizer(sentence)[:max_len-2]\n",
    "    ids = [vocab.get('<sos>')] + [vocab.get(tok, vocab['<unk>']) for tok in tokens] + [vocab.get('<eos>')]\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "# ===============================\n",
    "# üì¶ Dataset t√πy ch·ªânh (thay torchtext Dataset)\n",
    "# ===============================\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_texts, trg_texts, src_vocab, trg_vocab, src_tokenizer, trg_tokenizer):\n",
    "        self.src_texts = src_texts\n",
    "        self.trg_texts = trg_texts\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_tensor = encode_sentence(self.src_texts[idx], self.src_vocab, self.src_tokenizer)\n",
    "        trg_tensor = encode_sentence(self.trg_texts[idx], self.trg_vocab, self.trg_tokenizer)\n",
    "        return src_tensor, trg_tensor\n",
    "\n",
    "# ===============================\n",
    "# üßÆ Collate function ƒë·ªÉ padding batch\n",
    "# ===============================\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_padded = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    trg_padded = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    return src_padded, trg_padded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bb8c8c",
   "metadata": {},
   "source": [
    "X·ª¨ L√ù D·ªÆ LI·ªÜU C∆† B·∫¢N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb539144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y t·ªáp. Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n: data/data/train.en ho·∫∑c data/data/train.fr\n",
      "‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y t·ªáp. Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n: data/data/val.en ho·∫∑c data/data/val.fr\n",
      "‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y t·ªáp. Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n: data/data/test.en ho·∫∑c data/data/test.fr\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "‚ùóKh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu ƒë√†o t·∫°o. Vui l√≤ng ki·ªÉm tra th∆∞ m·ª•c 'data/' v√† c√°c t·ªáp Multi30K.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     63\u001b[39m test_data  = read_pairs(\u001b[33m'\u001b[39m\u001b[33mdata/test.en\u001b[39m\u001b[33m'\u001b[39m,  \u001b[33m'\u001b[39m\u001b[33mdata/test.fr\u001b[39m\u001b[33m'\u001b[39m,  data_dir=DATA_DIR, max_len=MAX_SENTENCE_LEN)\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m train_data:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m‚ùóKh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu ƒë√†o t·∫°o. Vui l√≤ng ki·ªÉm tra th∆∞ m·ª•c \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdata/\u001b[39m\u001b[33m'\u001b[39m\u001b[33m v√† c√°c t·ªáp Multi30K.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: ‚ùóKh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu ƒë√†o t·∫°o. Vui l√≤ng ki·ªÉm tra th∆∞ m·ª•c 'data/' v√† c√°c t·ªáp Multi30K."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "\n",
    "# ===============================\n",
    "# üß† Tokenizers (thay cho torchtext)\n",
    "# ===============================\n",
    "# N·∫øu ch∆∞a t·∫£i, m·ªü terminal v√† ch·∫°y:\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download fr_core_news_sm\n",
    "\n",
    "# T·∫£i m√¥ h√¨nh spaCy\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "spacy_fr = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# H√†m token h√≥a th·ªß c√¥ng\n",
    "def en_tokenizer(text):\n",
    "    \"\"\"Tokenizer ti·∫øng Anh\"\"\"\n",
    "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def fr_tokenizer(text):\n",
    "    \"\"\"Tokenizer ti·∫øng Ph√°p\"\"\"\n",
    "    return [tok.text.lower() for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# üìÇ H√†m ƒë·ªçc d·ªØ li·ªáu Multi30K\n",
    "# ===============================\n",
    "def read_pairs(file_en, file_fr, data_dir='data/', max_len=50):\n",
    "    \"\"\"\n",
    "    ƒê·ªçc d·ªØ li·ªáu t·ª´ t·ªáp song ng·ªØ (Anh - Ph√°p),\n",
    "    l·ªçc c√¢u qu√° d√†i, v√† tr·∫£ v·ªÅ danh s√°ch (en, fr).\n",
    "    \"\"\"\n",
    "    path_en = os.path.join(data_dir, file_en)\n",
    "    path_fr = os.path.join(data_dir, file_fr)\n",
    "    data = []\n",
    "\n",
    "    try:\n",
    "        with open(path_en, 'r', encoding='utf-8') as f_en, open(path_fr, 'r', encoding='utf-8') as f_fr:\n",
    "            for en_line, fr_line in zip(f_en, f_fr):\n",
    "                en_clean = en_line.strip()\n",
    "                fr_clean = fr_line.strip()\n",
    "\n",
    "                # B·ªè c√¢u tr·ªëng ho·∫∑c qu√° d√†i\n",
    "                if 0 < len(en_clean.split()) <= max_len and 0 < len(fr_clean.split()) <= max_len:\n",
    "                    data.append((en_clean, fr_clean))\n",
    "\n",
    "        print(f\"‚úÖ ƒê√£ t·∫£i {len(data)} c·∫∑p c√¢u t·ª´ {file_en} & {file_fr}\")\n",
    "        return data\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y t·ªáp. Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n: {path_en} ho·∫∑c {path_fr}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# üß© T·∫£i d·ªØ li·ªáu th·ª±c t·∫ø\n",
    "# ===============================\n",
    "DATA_DIR = 'data/'\n",
    "MAX_SENTENCE_LEN = 50\n",
    "\n",
    "train_data = read_pairs('data/train.en', 'data/train.fr', data_dir=DATA_DIR, max_len=MAX_SENTENCE_LEN)\n",
    "val_data   = read_pairs('data/val.en',   'data/val.fr',   data_dir=DATA_DIR, max_len=MAX_SENTENCE_LEN)\n",
    "test_data  = read_pairs('data/test.en',  'data/test.fr',  data_dir=DATA_DIR, max_len=MAX_SENTENCE_LEN)\n",
    "\n",
    "if not train_data:\n",
    "    raise RuntimeError(\"‚ùóKh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu ƒë√†o t·∫°o. Vui l√≤ng ki·ªÉm tra th∆∞ m·ª•c 'data/' v√† c√°c t·ªáp Multi30K.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257146a7",
   "metadata": {},
   "source": [
    "X√ÇY D·ª∞NG T·ª™ V·ª∞NG V√Ä DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d42fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- X√¢y d·ª±ng T·ª´ v·ª±ng ---\n",
    "def yield_tokens(data_iter, tokenizer, src_idx=0):\n",
    "    for pair in data_iter:\n",
    "        yield tokenizer(pair[src_idx])\n",
    "\n",
    "en_vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(train_data, en_tokenizer, 0),\n",
    "    min_freq=1, specials=special_tokens, max_tokens=MAX_VOCAB_SIZE\n",
    ")\n",
    "en_vocab.set_default_index(UNK_IDX)\n",
    "\n",
    "fr_vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(train_data, fr_tokenizer, 1),\n",
    "    min_freq=1, specials=special_tokens, max_tokens=MAX_VOCAB_SIZE\n",
    ")\n",
    "fr_vocab.set_default_index(UNK_IDX)\n",
    "\n",
    "print(f\"K√≠ch th∆∞·ªõc t·ª´ v·ª±ng EN: {len(en_vocab)}, FR: {len(fr_vocab)}\")\n",
    "\n",
    "# --- Chuy·ªÉn ƒë·ªïi Chu·ªói th√†nh Index ---\n",
    "def data_process(data):\n",
    "    data_index = []\n",
    "    for en_sentence, fr_sentence in data:\n",
    "        en_tensor = torch.tensor(en_vocab(en_tokenizer(en_sentence)), dtype=torch.long)\n",
    "        fr_tensor = torch.tensor([SOS_IDX] + fr_vocab(fr_tokenizer(fr_sentence)) + [EOS_IDX], dtype=torch.long)\n",
    "        data_index.append((en_tensor, fr_tensor))\n",
    "    return data_index\n",
    "\n",
    "train_data_indices = data_process(train_data)\n",
    "val_data_indices = data_process(val_data)\n",
    "test_data_indices = data_process(test_data)\n",
    "\n",
    "# --- Custom Dataset v√† Collate Function ---\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # S·∫Øp x·∫øp batch v√† Padding\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    en_batch, fr_batch = zip(*batch)\n",
    "    en_lengths = torch.tensor([len(seq) for seq in en_batch], dtype=torch.long)\n",
    "    en_padded = pad_sequence(en_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    fr_padded = pad_sequence(fr_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    \n",
    "    return en_padded.to(device), en_lengths.to(device), fr_padded.to(device)\n",
    "\n",
    "train_iterator = DataLoader(TranslationDataset(train_data_indices), batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "valid_iterator = DataLoader(TranslationDataset(val_data_indices), batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "test_iterator = DataLoader(TranslationDataset(test_data_indices), batch_size=1, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17491e5",
   "metadata": {},
   "source": [
    "ƒê·ªäNH NGHƒ®A M√î H√åNH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753307a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # ... (code Encoder nh∆∞ trong utils.py)\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, src, src_len):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        packed_embedded = pack_padded_sequence(embedded, src_len.cpu(), batch_first=True, enforce_sorted=True)\n",
    "        _, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    # ... (code Decoder nh∆∞ trong utils.py)\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(1) \n",
    "        embedded = self.dropout(self.embedding(input)) \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        output_1d = output.squeeze(1) \n",
    "        prediction = self.fc_out(output_1d)\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    # ... (code Seq2Seq nh∆∞ trong utils.py)\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert encoder.hid_dim == decoder.hid_dim\n",
    "        assert encoder.n_layers == decoder.n_layers\n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        hidden, cell = self.encoder(src, src_len)\n",
    "        input = trg[:, 0]\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t] = output\n",
    "            top1 = output.argmax(1) \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a713f5",
   "metadata": {},
   "source": [
    "KH·ªûI T·∫†O M√î H√åNH V√Ä OPTIMIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eaa329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tham s·ªë m√¥ h√¨nh\n",
    "INPUT_DIM = len(en_vocab)\n",
    "OUTPUT_DIM = len(fr_vocab)\n",
    "EMB_DIM = 256 \n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    # Kh·ªüi t·∫°o tr·ªçng s·ªë\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)\n",
    "print(\"ƒê√£ kh·ªüi t·∫°o m√¥ h√¨nh v√† tr·ªçng s·ªë.\")\n",
    "\n",
    "# Optimizer & Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n",
    "CLIP = 1.0 # Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b9f1d",
   "metadata": {},
   "source": [
    "ƒê·ªäNH NGHƒ®A H√ÄM HU·∫§N LUY·ªÜN V√Ä ƒê√ÅNH GI√Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729dec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \"\"\"Hu·∫•n luy·ªán m√¥ h√¨nh trong m·ªôt epoch.\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (src, src_len, trg) in enumerate(tqdm(iterator, desc=\"Training\")):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, src_len, trg, teacher_forcing_ratio=0.5)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"ƒê√°nh gi√° m√¥ h√¨nh (Validation) v·ªõi Teacher Forcing = 0.\"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (src, src_len, trg) in enumerate(tqdm(iterator, desc=\"Evaluating\")):\n",
    "            output = model(src, src_len, trg, 0) # Teacher Forcing Ratio = 0\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ae1974",
   "metadata": {},
   "source": [
    "V√íNG L·∫∂P HU·∫§N LUY·ªÜN CH√çNH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76965a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 20\n",
    "best_valid_loss = float('inf')\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "patience = 0 \n",
    "\n",
    "if not os.path.exists('checkpoints'):\n",
    "    os.makedirs('checkpoints')\n",
    "\n",
    "print(\"--- B·∫Øt ƒë·∫ßu Hu·∫•n luy·ªán ---\")\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'checkpoints/best_model.pth')\n",
    "        patience = 0\n",
    "        print(f'*** Epoch {epoch+1:02}: L∆∞u m√¥ h√¨nh t·ªët nh·∫•t! ***')\n",
    "    else:\n",
    "        patience += 1\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Patience: {patience}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):7.3f}')\n",
    "    print(f'\\tValid Loss: {valid_loss:.3f} | Valid PPL: {np.exp(valid_loss):7.3f}')\n",
    "\n",
    "    if patience >= 3:\n",
    "        print(f\"\\n--- D·ª´ng s·ªõm t·∫°i Epoch {epoch+1} ---\")\n",
    "        break\n",
    "\n",
    "# T·∫£i m√¥ h√¨nh t·ªët nh·∫•t ƒë·ªÉ s·ª≠ d·ª•ng cho Inference\n",
    "model.load_state_dict(torch.load('checkpoints/best_model.pth'))\n",
    "print(\"\\nƒê√£ t·∫£i m√¥ h√¨nh t·ªët nh·∫•t.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1406d80",
   "metadata": {},
   "source": [
    "H√ÄM D·ªäCH (INFERENCE) V√Ä ƒê√ÅNH GI√Å BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f985c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, en_vocab, fr_vocab, en_tokenizer, model, device, max_len=50):\n",
    "    \"\"\"Th·ª±c hi·ªán d·ªãch c√¢u ƒë∆°n b·∫±ng Greedy Decoding.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    tokens = en_tokenizer(sentence)\n",
    "    src_indexes = [en_vocab[token] for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device) \n",
    "    src_len = torch.LongTensor([len(src_indexes)]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tensor, src_len)\n",
    "\n",
    "    input = torch.LongTensor([SOS_IDX]).to(device) \n",
    "    trg_indexes = []\n",
    "\n",
    "    for t in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(input, hidden, cell)\n",
    "        \n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == EOS_IDX:\n",
    "            break\n",
    "            \n",
    "        input = torch.LongTensor([pred_token]).to(device)\n",
    "        \n",
    "    trg_tokens = [fr_vocab.lookup_token(i) for i in trg_indexes]\n",
    "    \n",
    "    translation = ' '.join(trg_tokens[1:-1])\n",
    "    return translation\n",
    "\n",
    "def calculate_bleu_score(test_data_indices, model, device):\n",
    "    \"\"\"T√≠nh to√°n BLEU Score tr√™n to√†n b·ªô t·∫≠p test.\"\"\"\n",
    "    trgs = []\n",
    "    prds = []\n",
    "    \n",
    "    for src_tensor, trg_tensor in tqdm(test_data_indices, desc=\"T√≠nh BLEU\"):\n",
    "        \n",
    "        # Chuy·ªÉn tensor th√†nh chu·ªói ti·∫øng Anh ƒë·ªÉ d√πng h√†m translate\n",
    "        src_sentence = [en_vocab.lookup_token(i.item()) for i in src_tensor]\n",
    "        sentence_str = ' '.join([t for t in src_sentence if t not in special_tokens])\n",
    "        predicted_str = translate(sentence_str, en_vocab, fr_vocab, en_tokenizer, model, device)\n",
    "        predicted_tokens = predicted_str.split()\n",
    "        \n",
    "        # Ground Truth\n",
    "        target_tokens = [fr_vocab.lookup_token(i.item()) for i in trg_tensor]\n",
    "        clean_target = [t for t in target_tokens if t not in special_tokens]\n",
    "        \n",
    "        trgs.append([clean_target]) \n",
    "        prds.append(predicted_tokens)\n",
    "\n",
    "    bleu_score = corpus_bleu(trgs, prds)\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736c6274",
   "metadata": {},
   "source": [
    "K·∫æT QU·∫¢ CU·ªêI C√ôNG V√Ä ƒê·ªí TH·ªä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba264b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√≠nh to√°n BLEU Score\n",
    "print(\"\\n--- 6.5. ƒê√°nh gi√° ---\")\n",
    "bleu_score = calculate_bleu_score(test_data_indices, model, device)\n",
    "print(f\"BLEU Score tr√™n t·∫≠p Test: {bleu_score*100:.2f}\")\n",
    "\n",
    "# V√≠ d·ª• D·ªãch\n",
    "print(\"\\n--- V√≠ d·ª• D·ªãch ---\")\n",
    "example_sentence = \"A man is eating spaghetti.\"\n",
    "translation_output = translate(example_sentence, en_vocab, fr_vocab, en_tokenizer, model, device)\n",
    "print(f\"C√¢u ngu·ªìn (EN): {example_sentence}\")\n",
    "print(f\"D·ªãch (FR): {translation_output}\")\n",
    "\n",
    "\n",
    "# ƒê·ªì th·ªã Loss\n",
    "if train_losses and valid_losses:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(valid_losses, label='Validation Loss')\n",
    "    plt.title('Train vs. Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # L∆∞u bi·ªÉu ƒë·ªì (B·∫Øt bu·ªôc cho b√°o c√°o)\n",
    "    plt.savefig('checkpoints/train_val_loss.png') \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
